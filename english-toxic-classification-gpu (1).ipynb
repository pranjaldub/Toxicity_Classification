{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/toxic-model/toxic.h5\n",
      "/kaggle/input/saved-model/toxic_rnn.h5\n",
      "/kaggle/input/glove-vector/glove.6B.100d.txt\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "import tensorflow as tf\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "#try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "#    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#    print('Running on TPU ', tpu.master())\n",
    "#except ValueError:\n",
    " #   tpu = None\n",
    "\n",
    "#if tpu:\n",
    " #   tf.config.experimental_connect_to_cluster(tpu)\n",
    "  #  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "   # strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "#else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    " #   strategy = tf.distribute.get_strategy()\n",
    "\n",
    "#print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "#AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "MODEL = 'PD_TOXIC_X_CLASS'\n",
    "# Set your own project id here\n",
    "#PROJECT_ID = 'pd@toxic1234'\n",
    "#from google.cloud import storage\n",
    "#storage_client = storage.Client(project=PROJECT_ID)\n",
    "#from google.cloud import bigquery\n",
    "#bigquery_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,numpy as np,pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM,Bidirectional,Dense,GlobalMaxPooling1D,Embedding,Input,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab in glove 400000\n"
     ]
    }
   ],
   "source": [
    "#train data and test labels \n",
    "train = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "train.head()\n",
    "#load word vectors\n",
    "word2vec = {}\n",
    "with open(os.path.join('/kaggle/input/glove-vector/glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray((values [1:]))\n",
    "        word2vec[word] = vec\n",
    "print('total vocab in glove',len(word2vec))\n",
    "        \n",
    "        \n",
    "        \n",
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n",
    "\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223549, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223544</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223545</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223546</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223547</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223548</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "223544  fff8f64043129fa2  :Jerome, I see you never got around to this…! ...   \n",
       "223545  fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...   \n",
       "223546  fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...   \n",
       "223547  fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...   \n",
       "223548  fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "223544      0             0        0       0       0              0  \n",
       "223545      0             0        0       0       0              0  \n",
       "223546      0             0        0       0       0              0  \n",
       "223547      1             0        1       0       1              0  \n",
       "223548      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20\n",
    "max_voc_size = 400000\n",
    "embed_dim = 100\n",
    "val_split = 0.2\n",
    "batch_size = 128\n",
    "epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\"]\n",
      "[[0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#extract sentences\n",
    "sentences = train['comment_text'].fillna('DUMMY_VALUES').values\n",
    "#target\n",
    "labels = ['toxic' ,'severe_toxic' ,'obscene' ,'threat' ,'insult' ,'identity_hate']\n",
    "target  = train[labels].values\n",
    "print(sentences[1:2])\n",
    "print(target[1:2])\n",
    "\n",
    "#SENTENCE FOR TEST\n",
    "#sentences_test = test['comment_text'].fillna('DUMMY_VALUES').values\n",
    "\n",
    "#SENTENCE FOR validate\n",
    "#sentences_validate = valid['comment_text'].fillna('DUMMY_VALUES').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_voc_size)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "#sequences test\n",
    "#sequences_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "#sequences validate\n",
    "#sequences_validate = tokenizer.texts_to_sequences(sentences_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223549\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2idx\n",
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding \n",
    "padded_seq = pad_sequences(sequences,maxlen=max_seq_len)\n",
    "\n",
    "#padding test data\n",
    "#padded_seq_test = pad_sequences(sequences_test , maxlen = max_seq_len)\n",
    "\n",
    "#padding the validate data\n",
    "#padded_seq_validate = pad_sequences(sequences_validate , maxlen = max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223549, 20)\n"
     ]
    }
   ],
   "source": [
    "print(padded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "num_words = min(max_voc_size,len(word2idx)+1)\n",
    "embed_matrix = np.zeros((num_words,embed_dim))\n",
    "for word , i in word2idx.items():\n",
    "    if i < max_voc_size:\n",
    "        #getting word vector\n",
    "        word_vec = word2vec.get(word)\n",
    "        if word_vec is not None:\n",
    "            embed_matrix[i]=word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer\n",
    "embed_layer = Embedding(num_words,embed_dim,\n",
    "                        weights = [embed_matrix],\n",
    "                       input_length=max_seq_len,\n",
    "                       trainable = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 20, 20)            6005160   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 20)            3280      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 6, 40)             6560      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 246       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 6,015,246\n",
      "Trainable params: 6,015,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "# detect and init the TPU\n",
    "#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#tf.config.experimental_connect_to_cluster(tpu)\n",
    "#tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "#with tpu_strategy.scope():\n",
    "    \n",
    "input_ = Input(shape = (20,))\n",
    "# = embed_layer(input_)\n",
    "x = Embedding(num_words , 20,input_length=max_seq_len)(input_)\n",
    "x = LSTM(20,return_sequences=True)(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Bidirectional(LSTM(20,return_sequences = True))(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(6)(x)\n",
    "out = Activation('sigmoid')(x)\n",
    "model = Model(input_,out)\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = Adam(lr = 0.001),\n",
    "             metrics = ['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing callbacks\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "checkpoint = ModelCheckpoint(\"toxic_rnn.h5\",\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only=True,\n",
    "                            verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                             factor=0.1,\n",
    "                             patience=3,\n",
    "                             verbose=1,\n",
    "                             min_delta=0.0001)\n",
    "#putting callbacks in callbacks list\n",
    "callbacks = [checkpoint,early_stopping,reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178839 samples, validate on 44710 samples\n",
      "Epoch 1/5\n",
      "178839/178839 [==============================] - 61s 340us/step - loss: 0.1207 - categorical_accuracy: 0.9726 - val_loss: 0.0925 - val_categorical_accuracy: 0.9969\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09254, saving model to toxic_rnn.h5\n",
      "Epoch 2/5\n",
      "178839/178839 [==============================] - 56s 312us/step - loss: 0.0707 - categorical_accuracy: 0.9945 - val_loss: 0.0937 - val_categorical_accuracy: 0.9974\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.09254\n",
      "Epoch 3/5\n",
      "178839/178839 [==============================] - 55s 306us/step - loss: 0.0629 - categorical_accuracy: 0.9946 - val_loss: 0.1021 - val_categorical_accuracy: 0.9974\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09254\n",
      "Epoch 4/5\n",
      "178839/178839 [==============================] - 55s 305us/step - loss: 0.0571 - categorical_accuracy: 0.9946 - val_loss: 0.1071 - val_categorical_accuracy: 0.9974\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09254\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(padded_seq,target,batch_size=batch_size,epochs=epoch,validation_split = 0.2 ,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/toxic_softmax3_notglove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (3.17.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model('/kaggle/working/toxic_softmax3_notglove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 20, 20)            6005160   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 20)            3280      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 6, 40)             6560      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 246       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 6,015,246\n",
      "Trainable params: 6,015,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_new = train['comment_text'].fillna('DUMMY_VALUES').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer4 = tokenizer_from_json(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer2 = Tokenizer(num_words = 20000)\n",
    "#tokenizer2.fit_on_texts(sentences_new)\n",
    "sequences_cus = tokenizer4.texts_to_sequences(sentences_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4dc3a2729879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFUCKED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer2' is not defined"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer2.word_index\n",
    "print(word2idx[FUCKED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = ['COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK']\n",
    "s = ['I amm happy for you brother']\n",
    "s2 = ['i will kill you motherfucker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 473, 12, 6, 2144]]\n",
      "[[7, 44, 795, 6, 2712]]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer2 = Tokenizer(num_words =  20000)\n",
    "#tokenizer.fit_on_texts(s)\n",
    "sequences_custom = tokenizer4.texts_to_sequences(s)\n",
    "sequences_custom2 = tokenizer4.texts_to_sequences(s2)\n",
    "print(sequences_custom)\n",
    "print(sequences_custom2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq_cus = pad_sequences(sequences_custom,maxlen=20,padding  = 'post')\n",
    "padded_seq_cus_2 = pad_sequences(sequences_custom2,maxlen=20,padding  = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7   44  795    6 2712    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 20)\n",
      "[[   7  473   12    6 2144    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "print(padded_seq_cus_2)\n",
    "print(padded_seq_cus_2.shape)\n",
    "print(padded_seq_cus)\n",
    "print(padded_seq_cus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model2.predict(padded_seq_cus)\n",
    "x2 = model2.predict(padded_seq_cus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04101384 0.0014082  0.01564648 0.00086248 0.01461239 0.00354151]]\n",
      "[[0.8083033  0.09517158 0.6449795  0.04238441 0.54048836 0.11803702]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence is 0.041013844311237335 percent toxic of category toxic \n",
      "sentence is 0.001408196403644979 percent toxic of category severe \n",
      "sentence is 0.01564648374915123 percent toxic of category obscene \n",
      "sentence is 0.0008624771144241095 percent toxic of category threat \n",
      "sentence is 0.014612388797104359 percent toxic of category insult \n",
      "sentence is 0.003541512181982398 percent toxic of category identity_hate \n",
      "None\n",
      "sentence is 0.8083032965660095 percent toxic of category toxic \n",
      "sentence is 0.09517157822847366 percent toxic of category severe \n",
      "sentence is 0.6449794769287109 percent toxic of category obscene \n",
      "sentence is 0.04238441213965416 percent toxic of category threat \n",
      "sentence is 0.5404883623123169 percent toxic of category insult \n",
      "sentence is 0.11803702265024185 percent toxic of category identity_hate \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def category(arr):\n",
    "    label = ['toxic','severe','obscene','threat','insult','identity_hate']\n",
    "    for a in arr:\n",
    "        for x in range(6):\n",
    "            print ('sentence is {} percent toxic of category {} '.format(a[x],label[x]))\n",
    "print(category(x))\n",
    "print(category(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaded(name):\n",
    "    from keras.preprocessing.text import tokenizer_from_json\n",
    "    with open(name) as f:\n",
    "        data = json.load(f)\n",
    "        tokenizer_loaded = tokenizer_from_json(data)\n",
    "        return tokenizer_loaded\n",
    "def load_saved_model(location):\n",
    "     model_infer = load_model('/kaggle/working/toxic_softmax3_notglove.h5')\n",
    "     return model_infer\n",
    "\n",
    "def pipeline(lis):\n",
    "    name = 'tokenizer.json'\n",
    "    location = '/kaggle/working/toxic_softmax3_notglove.h5'\n",
    "    model_infer = load_saved_model(location)\n",
    "    tokenizer_pipe = loaded(name)\n",
    "    sequences_custom_pipe = tokenizer_pipe.texts_to_sequences(lis)\n",
    "    padded_seq_cus_pipe = pad_sequences(sequences_custom_pipe,maxlen=20,padding  = 'post')\n",
    "    pred = model_infer.predict(padded_seq_cus_pipe)\n",
    "    category(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence is 0.5331634283065796 percent toxic of category toxic \n",
      "sentence is 0.029372794553637505 percent toxic of category severe \n",
      "sentence is 0.28596416115760803 percent toxic of category obscene \n",
      "sentence is 0.014876600354909897 percent toxic of category threat \n",
      "sentence is 0.26000919938087463 percent toxic of category insult \n",
      "sentence is 0.04867182672023773 percent toxic of category identity_hate \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence is 0.8284076452255249 percent toxic of category toxic \n",
      "sentence is 0.11493822187185287 percent toxic of category severe \n",
      "sentence is 0.6836121678352356 percent toxic of category obscene \n",
      "sentence is 0.058817800134420395 percent toxic of category threat \n",
      "sentence is 0.583493173122406 percent toxic of category insult \n",
      "sentence is 0.13760221004486084 percent toxic of category identity_hate \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence is 0.04195234552025795 percent toxic of category toxic \n",
      "sentence is 0.0014813330490142107 percent toxic of category severe \n",
      "sentence is 0.015989087522029877 percent toxic of category obscene \n",
      "sentence is 0.0009036391857080162 percent toxic of category threat \n",
      "sentence is 0.014486114494502544 percent toxic of category insult \n",
      "sentence is 0.003547986736521125 percent toxic of category identity_hate \n"
     ]
    }
   ],
   "source": [
    "lis = ['i want to kill you till death']\n",
    "lis2 = ['motherfucker fuck you man']\n",
    "lis3 = ['i am happy that you got the job']\n",
    "pipeline(lis)\n",
    "pipeline(lis2)\n",
    "pipeline(lis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
